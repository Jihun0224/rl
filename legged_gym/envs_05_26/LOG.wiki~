
task-specific domain randomization : reduce too large randomization space

* observation에 target 자세 추가
* 
* 
* network size 변경 현재 actor&critic [32]	-> [128,64]
* 
*
= May 26 =
* tracking controller도 잘 동작하는 모습. 
* 'May23_11-09-11_' 시도에서 target을 옮겼을 때, 나름 의미있게 움직이는 것을 보임 = 성공적이지는 않음. 
  But, 기존에 오른손 움직임이 없던 관절이 움직이지 않는 모습을 보임..
  해당 state에서의 policy를 배우지 못한 것으로 보임
* target2에 대한 것을 학습
* panel init pose를 기존 init pose로 tracking controller parameter 학습
* default pose를 =0으로 학습
* target point에 대한 randomization을 진행하는 것이 필요해보임.
	- target point에 대한 observation도 추가 필요
= gpu0(0): = 

= gpu0(1): = 

= gpu1(0): = 

= gpu1(1): = 
* 
= May 23 =
* 'May22_17-19-01_와 May22_14-59-49_'결과 observation은 dof_pos, dof_vel, hand_pos, 3가지로도 동작함. 하지만 generalization이슈가 있음.
  initial pose를 변경하면 target pose에 영향을 끼침. 
* 'May22_14-53-32_' 결과 observation에 dof_pos, dof_vel, hand_pos에 추가로 target_dof, target_hand를 넣어도 동작함. 하지만 generalization이슈는 그대로 있음.
  initial pose를 변경하면 동작이 이상해 지나 target pose를 바꿔도 이전 target pose로 가는 모습을 보임. 
* observation test: observation을 변경한다고 하더라도...
* controller를 tracking controller로 구성
* noise_vec추가

[num_observations: 48]
                                    (self.dof_pos - self.default_dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.hand_pos-self.default_hand_pos-self.hand_target_pos) * self.obs_scales.hand_pos,
        episode_length_s = 1.0 # episode length in seconds
= gpu0(0): May23_11-12-24_= 
        if control_type=="P":
>>>
        if control_type=="P_target":
            torques = self.p_gains*(actions_scaled + self.default_dof_pos - self.dof_pos + self.dof_target_pos) - self.d_gains*self.dof_vel

= gpu0(1): May23_14-09-02_= 
import legged_gym.envs.rby1.target.target_left as target_data
        add_noise = True
        noise_level = 1.0 # scales other values
        class noise_scales(ManipulatedRobotCfg.noise.noise_scales):
            hand_pos = 0.0
>>>>
        add_noise = True
        noise_level = 1.0 # scales other values
        class noise_scales(ManipulatedRobotCfg.noise.noise_scales):
            hand_pos = 0.1

        if control_type=="P":
>>>
        if control_type=="P_target":
            torques = self.p_gains*(actions_scaled + self.default_dof_pos - self.dof_pos + self.dof_target_pos) - self.d_gains*self.dof_vel

= gpu1(0): May23_11-05-09_= 
        if control_type=="P":
>>>
        if control_type=="P_target":
            torques = self.p_gains*(actions_scaled + self.default_dof_pos - self.dof_pos - self.dof_target_pos) - self.d_gains*self.dof_vel

= gpu1(1): May23_11-09-11_= 
        if control_type=="P":
>>>
        if control_type=="P_target":
            torques = self.p_gains*(actions_scaled + self.default_dof_pos - self.dof_pos - self.dof_target_pos) - self.d_gains*self.dof_vel

        add_noise = True
>>>
        add_noise = False
* 
*
= May 22 =
* 일반화 되지 않음. observation 수정할 필요가 있어보임.
	(1) 개별 target_dof 제외하고, target_dof와 hand가 dof내에 들어가도록
	(2) 개별 target_dof와 target_hand가 각각 구별되도록.
* observation test: target에 대한 pose확인
* 손 위치에 대한 노이즈 수정
* 손에 observation noise 없앰
= gpu0(0):May22_14-51-29_(working) = 
task: panel -> target_left
        episode_length_s = 1.5 # episode length in seconds
                                    (self.dof_pos - self.default_dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos-self.default_hand_pos-self.hand_target_pos) * self.obs_scales.hand_pos,
>>>>
        num_privileged_obs = 48     # + hand_pos 6
                                    (self.dof_pos - self.default_dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.hand_pos - self.default_hand_pos - self.hand_target_pos) * self.obs_scales.hand_pos,

        noise_vec[0:self.num_actions] = noise_scales.dof_pos * noise_level * self.obs_scales.dof_pos
        noise_vec[self.num_actions:2*self.num_actions] = noise_scales.dof_vel * noise_level * self.obs_scales.dof_vel
        #noise_vec[2*self.num_actions:3*self.num_actions] = noise_scales.dof_target * noise_level * self.obs_scales.dof_target
        noise_vec[2*self.num_actions:2*self.num_actions+6] = 0. # noise_scales.hand_pos * noise_level * self.obs_scales.hand_pos
        noise_vec[2*self.num_actions+6:3*self.num_actions+6] = 0. # previous actions

= gpu1(0):May22_14-53-32_ (working) = 
task: panel -> target_left
        episode_length_s = 1.5 # episode length in seconds
        num_privileged_obs = 48     # + hand_pos 6
                                    (self.dof_pos - self.default_dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos-self.default_hand_pos-self.hand_target_pos) * self.obs_scales.hand_pos,
>>>>
                                    (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos-self.default_hand_pos) * self.obs_scales.hand_pos,
                                    (self.hand_target) * self.obs_scales.hand_target,

= gpu0(1):May22_17-19-01_(working) = 
import legged_gym.envs.rby1.target.target_left as target_data
        episode_length_s = 1.0 # episode length in seconds
        noise_vec = torch.zeros_like(self.obs_buf[0])
        self.add_noise = self.cfg.noise.add_noise
        noise_scales = self.cfg.noise.noise_scales
        noise_level = self.cfg.noise.noise_level
        noise_vec[0:self.num_actions] = noise_scales.dof_pos * noise_level * self.obs_scales.dof_pos
        noise_vec[self.num_actions:2*self.num_actions] = noise_scales.dof_vel * noise_level * self.obs_scales.dof_vel
        #noise_vec[2*self.num_actions:3*self.num_actions] = noise_scales.dof_target * noise_level * self.obs_scales.dof_target
        noise_vec[2*self.num_actions:2*self.num_actions+6] = 0. # noise_scales.hand_pos * noise_level * self.obs_scales.hand_pos
        noise_vec[2*self.num_actions+6:3*self.num_actions+6] = 0. # previous actions

              						(self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,
              						self.dof_vel * self.obs_scales.dof_vel,
                                    #(self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos) * self.obs_scales.hand_pos,

= gpu1(1):May22_14-59-49_ = 
        episode_length_s = 1.5 # episode length in seconds
        num_privileged_obs = 48		# except for the target pose
                                    (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos) * self.obs_scales.hand_pos,
>>>>
                                    (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    #(self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos) * self.obs_scales.hand_pos,
* 
* 
= May 21 =
* dof_acc의 reward scale을 올려보자: 가속도에 대한 penalty가 위치, 속도에 비해 큼.
* reset_buf 수정 1e-6이하에서 1e-3이하로 수정 진행
* episode length 조정으로 속도 조절
= gpu0(0): = 
task: panel -> target_left
              (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,
              self.dof_vel * self.obs_scales.dof_vel,
              (self.dof_target) * self.obs_scales.dof_target,
              (self.hand_pos) * self.obs_scales.hand_pos,
>>>>
              (self.dof_pos - self.default_dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
              self.dof_vel * self.obs_scales.dof_vel,
              (self.dof_target) * self.obs_scales.dof_target,
              (self.hand_pos-self.default_hand_pos-self.hand_target_pos) * self.obs_scales.hand_pos,

        num_envs = 1024
= gpu0(1): = 
task: panel -> target_left
              						(self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,
              						self.dof_vel * self.obs_scales.dof_vel,
              						(self.dof_target) * self.obs_scales.dof_target,
              						(self.hand_pos) * self.obs_scales.hand_pos,
>>>>
                                    (self.dof_pos - self.default_dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos-self.default_hand_pos-self.hand_target_pos) * self.obs_scales.hand_pos,
                                    (self.hand_target) * self.obs_scales.hand_pos,

= gpu1(0): = 
task: panel -> target_left
        self.reset_buf |= dist_to_target[:] < 1e-6
>>>
        self.reset_buf |= dist_to_target[:] < 1e-3

= gpu1(1): = 
task: panel -> target_left
        episode_length_s = 2.0 # episode length in seconds
>>>
        episode_length_s = 1.0 # episode length in seconds
*
= May 20 =
* target 자세에서 손이 떨리는 현상 해결 안됨().
	(1) add_noise=False로 학습
	(*) reward model action rate 추가
	- action_rate=-0.01 추가로 해결 되는 것으로 보임. 학습 결과 확인 필요 (개선 확인 완료).
* with action_rate 떨림은 사라졌으나 마지막 관절을 쭉 뻗지 않는 현상
	(*) wrist reward model 추가
* .
= gpu0(0): = 
            torques = -0.1e-7
            dof_vel = -2e-4
            dof_acc = -2.0e-7
            target_pos_dist=-0.1
            target_hand_dist=-5.0
            action_rate = -0.0025
            wrist = -0.1

= gpu0(1): = 
import legged_gym.envs.rby1.target.target_left as target_data

            torques = -0.1e-7
            dof_vel = -2e-4
            dof_acc = -2.0e-7
            target_pos_dist=-0.1
            target_hand_dist=-5.0
            action_rate = -0.0025
            wrist = -0.1

= gpu1(0): = 
	init pose >>> panel init pose
		import legged_gym.envs.rby1.target.target_left as target_data

            torques = -0.1e-7
            dof_vel = -2e-4
            dof_acc = -3.0e-7
            target_pos_dist=-0.1
            target_hand_dist=-5.0
            action_rate = -0.0025
            * wrist = -0.1
             
= gpu1(1): May20_16-08-16_ = 
	init pose >>> target 0
		import legged_gym.envs.rby1.target.target_both as target_data

            torques = -0.1e-7
            dof_vel = -2e-4
            dof_acc = -3.0e-7
            target_pos_dist=-0.1
            target_hand_dist=-5.0
            action_rate = -0.0025
            wrist = -0.1


= May 19 =
* target 자세에서 손이 떨리는 현상 해결 안됨.
	- 그래픽 카드 2개 돌리는 것이 문제인가? 확인 필요
* 손 동작에 대한 weight를 더 주고 돌려보자.
* early termination 바꿔보자
* 손 동작에 대한 target_default를 observation에 추가
* num_envs를 1024로 작게 가져갔을 때, 그래픽 카드 1개씩 돌릴때 스무스한 동작? 확인 필요
= gpu0(0):May19_17-17-27_ = 
        self.obs_buf = torch.cat((
                                    (self.dof_pos - self.default_dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    #(self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos-self.hand_target_pos) * self.obs_scales.hand_pos,
                                    #(self.hand_pos-self.default_hand_pos) * self.obs_scales.hand_pos,
                                    #(self.hand_target) * self.obs_scales.hand_pos,
                                    self.actions,
        self.reset_buf |= dist_to_target[:] < 0.0001
>>>
        self.reset_buf |= dist_to_target[:] < 1e-6

= gpu1(0):May19_17-19-54_ = 

        self.obs_buf = torch.cat((
                                    (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,
                                    self.dof_vel * self.obs_scales.dof_vel,
                                    (self.dof_target) * self.obs_scales.dof_target,
                                    (self.hand_pos-self.default_hand_pos) * self.obs_scales.hand_pos,
                                    (self.hand_target) * self.obs_scales.hand_pos,
                                    self.actions,
                                    ),dim=-1)
            observation = 62
>>>
            observation = 68

= gpu1(0): = 

= gpu1(1): = 
 

= May 16 =
* reaching 0.1이 0.5보다 성능이 좋음
* iteration; 5000 정도하면 뻗는 동작은 성능이 어느정도 나옴.
* A6000에 모델 두개씩 돌려도 정상 작동하는 듯 보임 (확인 필요)
* target 자세에서 손이 떨리는 현상 발생
	- (*) target 자세와 손 위치에 대한 noise 제거 (looks better)
	- 전체 observation에 대한 noise 제거 
	

= gpu0(0): May16_16-18-09_= 
            dof_acc = -3.0e-7
>>>
            dof_acc = -5.0e-7

= gpu0(1): May16_16-19-38_= 
			import legged_gym.envs.rby1.target.target_both as target_data
>>>
			import legged_gym.envs.rby1.target.target_left as target_data

= gpu1(0): May16_16-18-23_= 
	init pose >>> panel init pose

			import legged_gym.envs.rby1.target.target_both as target_data
>>>
			import legged_gym.envs.rby1.target.target_left as target_data

= gpu1(1): May16_16-23-12_= 
	init pose >>> target 0 pose

			import legged_gym.envs.rby1.target.target_both as target_data



[ May 15 - 16 ]
= gpu0:May15_14-02-11_ = 
	reward:
        hand_error = torch.square(self.hand_pos[:, :] - self.hand_target_pos[:,:])
        * >>hand_error = torch.square(self.hand_pos[:, :] - self.hand_target_pos)

	comp_obs:
        (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,

= gpu0(1):May15_15-07-08_ = 
	import legged_gym.envs.rby1.target.target_both as target_data
	* >> import legged_gym.envs.rby1.target.target_left as target_data

= gpu1:May15_14-23-24_ = 
	comp_obs:
         (self.dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
= gpu1(2):May15_15-09-25_ = 
	import legged_gym.envs.rby1.target.target_both as target_data
	* >> import legged_gym.envs.rby1.target.target_left as target_data
            reaching=-0.1
	* >>
            reaching=-0.5

= May 15 =
* obs_buf와  privileged_obs_buf에 다른 형태로 넣고 있었음.
	- 동일한 형태로 코드 수정
* reward_reaching의 스케일 조정 필요해 보임.
	- reward를 독점해 버림. 코드 수정

= gpu0:May15_14-02-11_ = 
	reward:
        hand_error = torch.square(self.hand_pos[:, :] - self.hand_target_pos[:,:])
        * >>hand_error = torch.square(self.hand_pos[:, :] - self.hand_target_pos)

	comp_obs:
        (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,

= gpu0(1):May15_15-07-08_ = 
	import legged_gym.envs.rby1.target.target_both as target_data
	* >> import legged_gym.envs.rby1.target.target_left as target_data

= gpu1:May15_14-23-24_ = 
	comp_obs:
         (self.dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
= gpu1(2):May15_15-09-25_ = 
	import legged_gym.envs.rby1.target.target_both as target_data
	* >> import legged_gym.envs.rby1.target.target_left as target_data
            reaching=-0.1
	* >>
            reaching=-0.5
= gpu0:May15_14-02-11_ = 
	reward:
        hand_error = torch.square(self.hand_pos[:, :] - self.hand_target_pos[:,:])
        * >>hand_error = torch.square(self.hand_pos[:, :] - self.hand_target_pos)

	comp_obs:
        (self.dof_pos - self.default_dof_pos) * self.obs_scales.dof_pos,

= gpu0(1):May15_15-07-08_ = 

= gpu1:May15_14-23-24_ = 
	comp_obs:
         (self.dof_pos - self.dof_target_pos) * self.obs_scales.dof_pos,
= gpu1(2):May15_15-09-25_ = 



= work =
            termination = -0.0
            lin_vel_z = -0.0
            torques = -0.1e-7
            dof_vel = -1e-4
            dof_acc = -3.0e-7
            reaching=-0.1
            goal=-5.0

=rby1.urdf=
default_joint_angles={
           'torso_0' : 0. ,   
           'torso_1' : 0.54,               
           'torso_2' : -1.256,         
           'torso_3' : 0.7,       
           'torso_4' : -0.2,     
           'torso_5' : 0,     

           'right_arm_0' : 0., 
           'right_arm_1' : -0.43, 
           'right_arm_2' :-0.174,                                       
           'right_arm_3' : -1.309,                                             
           'right_arm_4' : -0.,                              
           'right_arm_5' : 0.174,       
           'right_arm_6' : 0.,

           'left_arm_0' : 0., 
           'left_arm_1' : 0.43, 
           'left_arm_2' : 0.174,                                       
           'left_arm_3' : -1.309,                                             
           'left_arm_4' : -0.,                              
           'left_arm_5' : 0.174,       
           'left_arm_6' : 0.,

           'gripper_finger_r1' : 0.,                                             
           'gripper_finger_r2' : 0.,                              
           'gripper_finger_l1' : 0.,       
           'gripper_finger_l2' : 0.,
}

=rby1_upper.urdf=
default_joint_angles={
                'right_arm_0' : -0.477, 
                'right_arm_1' : -0.185,  
                'right_arm_2' : -0.128,                                        
                'right_arm_3' : -2.261,                                              
                'right_arm_4' : -0.182,                               
                'right_arm_5' : 1.702,       
                'right_arm_6' : 0.,

                'left_arm_0' : -0.477,  
                'left_arm_1' : 0.185, 
                'left_arm_2' : 0.128,                                        
                'left_arm_3' : -2.261,                                              
                'left_arm_4' : 0.182,                               
                'left_arm_5' : 1.702,       
                'left_arm_6' : 0.,
}
